<!DOCTYPE HTML>
<html lang="en">
<head>
    <title>Iterative Closest Point</title>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <link rel="stylesheet" href="../styles/blogs.css" type="text/css" charset="utf-8" />
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async
            src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
    </script>
</head>
<body>
<h1>Iterative Closest Point - ICP</h1>
<h2>Dataset</h2>
<p>
    <ul>
        <li>We first create two sets of toy dataset.</li>
        <li>We will call the first set of points the base points and will generate them using the latin hypercube sampling method</li>
        <div class="figure">
            <img width="437" height="355" alt="figure of inventors of BFGS"
            src="./figures/reference_points.png" />
            <p>Figure 1: set of base point.</p>
        </div>
          <li>As you can see there are no two points in a row or a column.</li>
          <li>Then we define 2 functions for Rotation and Translation and apply that to the base point to get a second set of points.</li>
          <div class="figure">
            <img width="437" height="355" alt="figure of inventors of BFGS"
            src="./figures/source_points.png" />
            <p>Figure 1: both sets of points.</p>
          </div>
    </ul>
</p>
<h2>What are we going to do?</h2>
<p>
    <ul>
        <li>Now we are going to find the transformation that mapped base points (reference point sets) to the second transformed set of points aka. source point set.</li>
        <li>In order to do that we have to find a first estimate of the pair of points and the transformation between them.</li>
        <li>A good way to initialize our guess is to find the closest point to each point in the reference point set.</li>
        <li>After finding the closest point, we have to calculate the transformation that can map the point in reference set to that closest point of last step.</li>
        <li>In order to do that we have to first formulate the problem into a least square minimization problem because it has many robust and fast methods to solve and easier too!</li>
    </ul>
</p>
<h2>Results</h2>
<p>
    <ul>
        <li>Here you can see the trajectory of the transformation for each point</li>
        <div class="figure">
            <img width="437" height="355" alt="figure of inventors of BFGS"
            src="./figures/transition.png" />
            <p>Figure 1: trajectory of transformation.</p>
        </div>
    </ul>
</p>
<h2>Why ICP</h2>
<p>
    <ul>
        <li>Member of the quasi-newton methods family</li>
        <li>The memory limited version of BFGS</li>
        <li>It is a method of approximating the hessian matrix</li>
        <li>The BFGS stores an nxn Hessian matrix</li>
        <li>L-BFGS stores only a few vectors</li>
        <li>Well suited for optimizations with many variables</li>
    </ul>
</p>
<div class="figure">
    <img width="437" height="355" alt="figure of inventors of BFGS"
    src="./bfgs.jpeg" />
    <p>Figure 1: Broyden, Fletcher, Goldfarb and Shanno from left to right.</p>
  </div>
<h2>How to use it?</h2>
Here we are going to use an implementation of L-BFGS in PyTorch to minimize the Rosenbrock function.
<h3>What is Rosenbrock function?</h3>
<p><a href="https://en.wikipedia.org/wiki/Rosenbrock_function">Rosenbrock function</a> is a non-convex function which its' global minima is
    inside a long narrow valley which is trivial to find, but converging to 
    the global minima inside that valley is a difficault task.
    \[f(x,y) = b(y-x^2)^2 + (a-x)^2\]
    where \(a\) and \(b\) are constants.
    The global minima is at \((a,a^2)\).
    It is mostly used as a <a href="https://en.wikipedia.org/wiki/Test_functions_for_optimization">benchmark for optimization algorithms</a>.
</p>
<div class="figure">
    <img width="437" height="355" alt="a contour plot of the Rosenbrock function"
    src="./rosenbrock.png" />
    <p>Figure 2: Rosenbrock function</p>
  </div>
<h3>What is a closure function?</h3>
<p>
    Some Optimizers in PyTorch such as Conjugate Gradient and LBFGS require a closure function to be passed to them.
    A closure function is a function that at first clears the gradients,
    computes the loss, 
    and reevaluates the function.
</p>
<pre>
    <code>
        def closure():
            optimizer.zero_grad()
            loss.backward()
            return loss.item()
    </code>
</pre>
<h3>Define a custom function in PyTorch</h3>
<p>In order to be able to use the optimizers implemented in pytorch to 
    minimize a function, we have to redefine that function as a class like 
    we do for different Neural Networks in PyTorch.
    <pre>
        <code>
class Rosenbrock(nn.Module):
    def __init__(self, a, b):
        super(Rosenbrock, self).__init__()
        # Initializing the Rosenbrock function
        self.a = a
        self.b = b
        # Optimization parameters are randomly initialized and
        # defined to be a nn.Parameter object.
        self.x = torch.nn.Parameter(torch.Tensor([-1.0]))
        self.y = torch.nn.Parameter(torch.Tensor([2.0]))

    def forward(self,):
        # Here is the function that is being optimized
        return (self.x - self.a) ** 2 + self.b * (self.y - self.x ** 2) ** 2
        </code>
    </pre>
    Note the difference between how we define constants and optimization variables.
    Then the optimizer can be initialized like this:
    <pre>
        <code>
optimizer = torch.optim.LBFGS(model.parameters(), lr=0.1, max_iter=5000)
        </code>
    </pre>
Here is the gif of the convergence steps:
    <div class="figure">
        <img width="437" height="355" alt="convergence animation"
        src="./convergence.gif" />
        <p>Figure 3: Convergence Steps (wait for it)</p>
      </div> 
you can find all the related codes inside this <a href="https://github.com/erfanhamdi/erfanhamdi.github.io/blob/main/blog_posts/l-bfgs/l-bfgs.py">github repository</a>.
</p>
</body>
</html>